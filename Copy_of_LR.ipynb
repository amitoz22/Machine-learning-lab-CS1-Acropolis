class MyLinearRegression:
    def __init__(self, weight=500 , bias=15, learning_rate=0.0005,
                 iterations=200):
        self.weight = weight
        self.bias = bias
        self.learning_rate = learning_rate
        self.iterations = iterations
        self.it= []
        self.cost_trend = []
        self.cost = 0

    def predict(self, x):
        predicted_set = []
        for i in range(len(x)):
            predicted_value = self.weight * x[i] + self.bias
            predicted_set.append(predicted_value)
        return predicted_set

    def cost_function(self, x, y):
        count = len(x)
        total_error = 0.0
        for i in range(count):
            total_error += (y[i] -(self.weight * x[i] +
                            self.bias)) ** 2
        return float(total_error) / (2 * count)

    def update_weights(self, x, y):
        weight_deriv = 0
        bias_deriv = 0
        count = len(x)

        for i in range(count):
            # Calculate partial derivatives
            # -2x(y - (mx + b))
            weight_deriv += -2 * x[i] * (y[i] -(self.weight * x[i] + self.bias))

            # -2(y - (mx + b))
            bias_deriv += -2 * (y[i] - (self.weight * x[i] +
                                self.bias))

        # We subtract because the derivatives point in direction of steepest
        # ascent
        self.weight -= (weight_deriv / count) * self.learning_rate
        self.bias -= (bias_deriv / count) * self.learning_rate

    def train(self, x, y):
        for i in range(self.iterations):
            self.update_weights(x, y)
            # Calculating cost
            self.cost = self.cost_function(x, y)
            self.cost_trend.append(self.cost)
            self.it.append(i)
           # if i % 10000 == 0:
            print("Iteration: {}\t Weight: {}\t Bias: {}\t Cost: {}".format(i, self.weight, self.bias, self.cost))
In [1]:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# intialise data of lists. 
data = {'Hours':[2.5,5.1,3.2,8.5,3.5,1.5,9.2,5.5,8.3,2.7,7.7,5.9,4.5,3.3,1.1,8.9,2.5,1.9,6.1,7.4,2.7,4.8,3.8,6.9,7.8], 
        'Scores':[21,47,27,75,30,20,88,60,81,25,85,62,41,42,17,95,30,24,67,69,30,54,35,76,86]} 
  
# Create DataFrame 
studentscores = pd.DataFrame(data) 
  
# Print the output. 
studentscores
Out[1]:
Hours	Scores
0	2.5	21
1	5.1	47
2	3.2	27
3	8.5	75
4	3.5	30
5	1.5	20
6	9.2	88
7	5.5	60
8	8.3	81
9	2.7	25
10	7.7	85
11	5.9	62
12	4.5	41
13	3.3	42
14	1.1	17
15	8.9	95
16	2.5	30
17	1.9	24
18	6.1	67
19	7.4	69
20	2.7	30
21	4.8	54
22	3.8	35
23	6.9	76
24	7.8	86
In [0]:

In [13]:
x=[2.5,5.1,3.2,8.5,3.5,1.5,9.2,5.5,8.3,2.7,7.7,5.9,4.5,3.3,1.1,8.9,2.5,1.9,6.1,7.4,2.7,4.8,3.8,6.9,7.8] 
y=[21,47,27,75,30,20,88,60,81,25,85,62,41,42,17,95,30,24,67,69,30,54,35,76,86]
plt.scatter(x,y,s=90)
plt.xlabel('x')
plt.ylabel('y')
plt.show()

In [4]:
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split



X = studentscores.iloc[:, :-1].values
y = studentscores.iloc[:, -1].values
X,y
Out[4]:
(array([[2.5],
        [5.1],
        [3.2],
        [8.5],
        [3.5],
        [1.5],
        [9.2],
        [5.5],
        [8.3],
        [2.7],
        [7.7],
        [5.9],
        [4.5],
        [3.3],
        [1.1],
        [8.9],
        [2.5],
        [1.9],
        [6.1],
        [7.4],
        [2.7],
        [4.8],
        [3.8],
        [6.9],
        [7.8]]),
 array([21, 47, 27, 75, 30, 20, 88, 60, 81, 25, 85, 62, 41, 42, 17, 95, 30,
        24, 67, 69, 30, 54, 35, 76, 86]))
In [21]:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Fitting Simple Linear Regression to the Training set
regressor = MyLinearRegression()
regressor.train(X_train, y_train)
print('Weight: ' + str(regressor.weight) + ' Bias: ' + str(regressor.bias))

# Predicting the Test set results
y_pred = regressor.predict(X_test)
Iteration: 0	 Weight: [483.44733]	 Bias: [12.41895]	 Cost: 3797094.901296486
Iteration: 1	 Weight: [467.46495031]	 Bias: [9.92721704]	 Cost: 3539957.419193694
Iteration: 2	 Weight: [452.03321062]	 Bias: [7.52172348]	 Cost: 3300234.851990161
Iteration: 3	 Weight: [437.13313772]	 Bias: [5.19949774]	 Cost: 3076747.7551048803
Iteration: 4	 Weight: [422.74641213]	 Bias: [2.9576706]	 Cost: 2868396.5631598933
Iteration: 5	 Weight: [408.85534562]	 Bias: [0.79347173]	 Cost: 2674156.1800719462
Iteration: 6	 Weight: [395.44285942]	 Bias: [-1.29577376]	 Cost: 2493070.9355362346
Iteration: 7	 Weight: [382.49246326]	 Bias: [-3.31264856]	 Cost: 2324249.8830879307
Iteration: 8	 Weight: [369.98823504]	 Bias: [-5.25964642]	 Cost: 2166862.416607752
Iteration: 9	 Weight: [357.91480132]	 Bias: [-7.13917513]	 Cost: 2020134.1837045879
Iteration: 10	 Weight: [346.25731834]	 Bias: [-8.95355951]	 Cost: 1883343.2758688577
Iteration: 11	 Weight: [335.00145383]	 Bias: [-10.7050443]	 Cost: 1755816.676651995
Iteration: 12	 Weight: [324.13336936]	 Bias: [-12.39579688]	 Cost: 1636926.950396947
Iteration: 13	 Weight: [313.63970332]	 Bias: [-14.02790993]	 Cost: 1526089.1552281033
Iteration: 14	 Weight: [303.50755451]	 Bias: [-15.60340407]	 Cost: 1422757.9651124314
Iteration: 15	 Weight: [293.72446624]	 Bias: [-17.12423025]	 Cost: 1326424.9868322434
Iteration: 16	 Weight: [284.27841106]	 Bias: [-18.59227222]	 Cost: 1236616.2586689792
Iteration: 17	 Weight: [275.15777594]	 Bias: [-20.00934883]	 Cost: 1152889.9184914273
Iteration: 18	 Weight: [266.35134797]	 Bias: [-21.37721622]	 Cost: 1074834.0297752838
Iteration: 19	 Weight: [257.84830064]	 Bias: [-22.69757007]	 Cost: 1002064.5548579575
Iteration: 20	 Weight: [249.63818047]	 Bias: [-23.97204759]	 Cost: 934223.4654569688
Iteration: 21	 Weight: [241.71089416]	 Bias: [-25.20222961]	 Cost: 870976.9811555988
Iteration: 22	 Weight: [234.05669621]	 Bias: [-26.38964247]	 Cost: 812013.9271890656
Iteration: 23	 Weight: [226.6661769]	 Bias: [-27.53575991]	 Cost: 757044.203451474
Iteration: 24	 Weight: [219.53025076]	 Bias: [-28.64200492]	 Cost: 705797.3571909743
Iteration: 25	 Weight: [212.64014535]	 Bias: [-29.70975143]	 Cost: 658021.2523707428
Iteration: 26	 Weight: [205.98739052]	 Bias: [-30.74032604]	 Cost: 613480.8291489754
Iteration: 27	 Weight: [199.56380796]	 Bias: [-31.73500964]	 Cost: 571956.9473744847
Iteration: 28	 Weight: [193.36150117]	 Bias: [-32.69503899]	 Cost: 533245.308407855
Iteration: 29	 Weight: [187.37284572]	 Bias: [-33.62160821]	 Cost: 497155.4499634603
Iteration: 30	 Weight: [181.5904799]	 Bias: [-34.51587032]	 Cost: 463509.80902693357
Iteration: 31	 Weight: [176.00729568]	 Bias: [-35.37893856]	 Cost: 432142.8482375969
Iteration: 32	 Weight: [170.61642992]	 Bias: [-36.21188785]	 Cost: 402900.24143762
Iteration: 33	 Weight: [165.41125598]	 Bias: [-37.01575606]	 Cost: 375638.11438077013
Iteration: 34	 Weight: [160.38537553]	 Bias: [-37.79154528]	 Cost: 350222.3368650131
Iteration: 35	 Weight: [155.53261074]	 Bias: [-38.5402231]	 Cost: 326527.86280622473
Iteration: 36	 Weight: [150.84699661]	 Bias: [-39.26272376]	 Cost: 304438.1150061475
Iteration: 37	 Weight: [146.32277371]	 Bias: [-39.9599493]	 Cost: 283844.41158762557
Iteration: 38	 Weight: [141.95438103]	 Bias: [-40.63277068]	 Cost: 264645.4312751559
Iteration: 39	 Weight: [137.73644918]	 Bias: [-41.28202887]	 Cost: 246746.71488990993
Iteration: 40	 Weight: [133.66379381]	 Bias: [-41.90853583]	 Cost: 230060.20060656453
Iteration: 41	 Weight: [129.73140917]	 Bias: [-42.51307558]	 Cost: 214503.79068538142
Iteration: 42	 Weight: [125.93446201]	 Bias: [-43.09640509]	 Cost: 200000.94754784342
Iteration: 43	 Weight: [122.26828561]	 Bias: [-43.65925526]	 Cost: 186480.31720851755
Iteration: 44	 Weight: [118.72837405]	 Bias: [-44.20233182]	 Cost: 173875.37821041836
Iteration: 45	 Weight: [115.31037668]	 Bias: [-44.72631617]	 Cost: 162124.11433661426
Iteration: 46	 Weight: [112.01009275]	 Bias: [-45.23186623]	 Cost: 151168.70948780817
Iteration: 47	 Weight: [108.82346624]	 Bias: [-45.71961725]	 Cost: 140955.263224674
Iteration: 48	 Weight: [105.74658093]	 Bias: [-46.1901826]	 Cost: 131433.5255754069
Iteration: 49	 Weight: [102.77565549]	 Bias: [-46.6441545]	 Cost: 122556.64980372836
Iteration: 50	 Weight: [99.90703891]	 Bias: [-47.08210478]	 Cost: 114280.9619209553
Iteration: 51	 Weight: [97.13720598]	 Bias: [-47.50458556]	 Cost: 106565.74580812121
Iteration: 52	 Weight: [94.46275295]	 Bias: [-47.91212993]	 Cost: 99373.04289094225
Iteration: 53	 Weight: [91.88039335]	 Bias: [-48.30525263]	 Cost: 92667.46538202054
Iteration: 54	 Weight: [89.38695396]	 Bias: [-48.68445063]	 Cost: 86416.02217142741
Iteration: 55	 Weight: [86.9793709]	 Bias: [-49.05020382]	 Cost: 80587.95650904212
Iteration: 56	 Weight: [84.65468585]	 Bias: [-49.40297552]	 Cost: 75154.59468003576
Iteration: 57	 Weight: [82.41004243]	 Bias: [-49.7432131]	 Cost: 70089.20492897641
Iteration: 58	 Weight: [80.24268268]	 Bias: [-50.07134851]	 Cost: 65366.86593845805
Iteration: 59	 Weight: [78.14994367]	 Bias: [-50.38779882]	 Cost: 60964.34421516076
Iteration: 60	 Weight: [76.12925423]	 Bias: [-50.69296672]	 Cost: 56859.9797800771
Iteration: 61	 Weight: [74.17813178]	 Bias: [-50.98724105]	 Cost: 53033.57960049609
Iteration: 62	 Weight: [72.29417928]	 Bias: [-51.27099722]	 Cost: 49466.31823942486
Iteration: 63	 Weight: [70.47508229]	 Bias: [-51.54459772]	 Cost: 46140.645233639574
Iteration: 64	 Weight: [68.71860612]	 Bias: [-51.80839256]	 Cost: 43040.19874466116
Iteration: 65	 Weight: [67.02259306]	 Bias: [-52.06271966]	 Cost: 40149.72505781558
Iteration: 66	 Weight: [65.38495977]	 Bias: [-52.30790533]	 Cost: 37455.00353330992
Iteration: 67	 Weight: [63.80369469]	 Bias: [-52.54426461]	 Cost: 34942.77664008108
Iteration: 68	 Weight: [62.27685556]	 Bias: [-52.77210171]	 Cost: 32600.68472818004
Iteration: 69	 Weight: [60.80256706]	 Bias: [-52.99171033]	 Cost: 30417.20521876947
Iteration: 70	 Weight: [59.37901847]	 Bias: [-53.20337407]	 Cost: 28381.595912546716
Iteration: 71	 Weight: [58.00446149]	 Bias: [-53.40736675]	 Cost: 26483.84213766727
Iteration: 72	 Weight: [56.67720802]	 Bias: [-53.60395276]	 Cost: 24714.607477134105
Iteration: 73	 Weight: [55.39562817]	 Bias: [-53.79338738]	 Cost: 23065.18783322981
Iteration: 74	 Weight: [54.15814817]	 Bias: [-53.97591708]	 Cost: 21527.4686029863
Iteration: 75	 Weight: [52.9632485]	 Bias: [-54.15177986]	 Cost: 20093.884753993825
Iteration: 76	 Weight: [51.809462]	 Bias: [-54.32120551]	 Cost: 18757.383604120336
Iteration: 77	 Weight: [50.69537206]	 Bias: [-54.48441588]	 Cost: 17511.390122016073
Iteration: 78	 Weight: [49.61961086]	 Bias: [-54.64162521]	 Cost: 16349.774577680271
Iteration: 79	 Weight: [48.58085775]	 Bias: [-54.79304035]	 Cost: 15266.822383929364
Iteration: 80	 Weight: [47.57783755]	 Bias: [-54.938861]	 Cost: 14257.205980385488
Iteration: 81	 Weight: [46.60931903]	 Bias: [-55.07928001]	 Cost: 13315.958621653339
Iteration: 82	 Weight: [45.67411341]	 Bias: [-55.21448356]	 Cost: 12438.449940721996
Iteration: 83	 Weight: [44.77107282]	 Bias: [-55.34465143]	 Cost: 11620.36316736274
Iteration: 84	 Weight: [43.89908899]	 Bias: [-55.46995721]	 Cost: 10857.673889436337
Iteration: 85	 Weight: [43.05709181]	 Bias: [-55.59056847]	 Cost: 10146.630252614497
Iteration: 86	 Weight: [42.24404805]	 Bias: [-55.70664707]	 Cost: 9483.734501097408
Iteration: 87	 Weight: [41.45896008]	 Bias: [-55.81834923]	 Cost: 8865.725768506749
Iteration: 88	 Weight: [40.70086465]	 Bias: [-55.92582583]	 Cost: 8289.564034284727
Iteration: 89	 Weight: [39.96883169]	 Bias: [-56.02922254]	 Cost: 7752.415166663983
Iteration: 90	 Weight: [39.26196319]	 Bias: [-56.12867999]	 Cost: 7251.63697861901
Iteration: 91	 Weight: [38.57939208]	 Bias: [-56.224334]	 Cost: 6784.766228193907
Iteration: 92	 Weight: [37.92028116]	 Bias: [-56.31631568]	 Cost: 6349.506499247441
Iteration: 93	 Weight: [37.28382208]	 Bias: [-56.40475164]	 Cost: 5943.716902988222
Iteration: 94	 Weight: [36.66923434]	 Bias: [-56.48976411]	 Cost: 5565.401544711023
Iteration: 95	 Weight: [36.07576433]	 Bias: [-56.57147114]	 Cost: 5212.699703910178
Iteration: 96	 Weight: [35.50268443]	 Bias: [-56.64998667]	 Cost: 4883.876679455764
Iteration: 97	 Weight: [34.94929207]	 Bias: [-56.72542075]	 Cost: 4577.315254790478
Iteration: 98	 Weight: [34.41490889]	 Bias: [-56.79787962]	 Cost: 4291.5077411555885
Iteration: 99	 Weight: [33.8988799]	 Bias: [-56.86746586]	 Cost: 4025.0485596983017
Iteration: 100	 Weight: [33.4005727]	 Bias: [-56.93427853]	 Cost: 3776.627325964215
Iteration: 101	 Weight: [32.91937665]	 Bias: [-56.99841325]	 Cost: 3545.022402750242
Iteration: 102	 Weight: [32.45470219]	 Bias: [-57.05996237]	 Cost: 3329.0948895977904
Iteration: 103	 Weight: [32.00598003]	 Bias: [-57.11901505]	 Cost: 3127.783019354219
Iteration: 104	 Weight: [31.57266052]	 Bias: [-57.17565737]	 Cost: 2940.0969342334492
Iteration: 105	 Weight: [31.15421296]	 Bias: [-57.22997245]	 Cost: 2765.113815673717
Iteration: 106	 Weight: [30.75012491]	 Bias: [-57.28204056]	 Cost: 2601.9733440311343
Iteration: 107	 Weight: [30.3599016]	 Bias: [-57.33193917]	 Cost: 2449.8734657706027
Iteration: 108	 Weight: [29.98306531]	 Bias: [-57.37974312]	 Cost: 2308.066447328444
Iteration: 109	 Weight: [29.61915478]	 Bias: [-57.42552464]	 Cost: 2175.8551962315905
Iteration: 110	 Weight: [29.26772464]	 Bias: [-57.46935348]	 Cost: 2052.589831373109
Iteration: 111	 Weight: [28.92834486]	 Bias: [-57.51129701]	 Cost: 1937.6644855696336
Iteration: 112	 Weight: [28.60060025]	 Bias: [-57.55142024]	 Cost: 1830.514324669183
Iteration: 113	 Weight: [28.2840899]	 Bias: [-57.58978596]	 Cost: 1730.6127685432268
Iteration: 114	 Weight: [27.97842673]	 Bias: [-57.62645481]	 Cost: 1637.46890029017
Iteration: 115	 Weight: [27.68323699]	 Bias: [-57.66148531]	 Cost: 1550.6250509034421
Iteration: 116	 Weight: [27.39815982]	 Bias: [-57.69493398]	 Cost: 1469.65454752062
Iteration: 117	 Weight: [27.12284677]	 Bias: [-57.72685541]	 Cost: 1394.1596141749137
Iteration: 118	 Weight: [26.85696142]	 Bias: [-57.75730227]	 Cost: 1323.7694147205982
Iteration: 119	 Weight: [26.60017893]	 Bias: [-57.78632544]	 Cost: 1258.1382283035086
Iteration: 120	 Weight: [26.35218565]	 Bias: [-57.81397406]	 Cost: 1196.9437483998363
Iteration: 121	 Weight: [26.11267876]	 Bias: [-57.84029554]	 Cost: 1139.885497054426
Iteration: 122	 Weight: [25.88136585]	 Bias: [-57.86533568]	 Cost: 1086.6833465165546
Iteration: 123	 Weight: [25.65796458]	 Bias: [-57.8891387]	 Cost: 1037.0761409995864
Iteration: 124	 Weight: [25.44220237]	 Bias: [-57.91174729]	 Cost: 990.8204117834882
Iteration: 125	 Weight: [25.23381601]	 Bias: [-57.93320269]	 Cost: 947.6891793384651
Iteration: 126	 Weight: [25.03255136]	 Bias: [-57.95354468]	 Cost: 907.4708365761211
Iteration: 127	 Weight: [24.83816303]	 Bias: [-57.9728117]	 Cost: 869.9681077336709
Iteration: 128	 Weight: [24.65041412]	 Bias: [-57.99104087]	 Cost: 834.997077768892
Iteration: 129	 Weight: [24.46907584]	 Bias: [-58.008268]	 Cost: 802.386287490386
Iteration: 130	 Weight: [24.29392733]	 Bias: [-58.02452769]	 Cost: 771.975889971152
Iteration: 131	 Weight: [24.12475532]	 Bias: [-58.03985334]	 Cost: 743.6168640949961
Iteration: 132	 Weight: [23.96135388]	 Bias: [-58.0542772]	 Cost: 717.1702813663944
Iteration: 133	 Weight: [23.80352419]	 Bias: [-58.06783042]	 Cost: 692.506622376468
Iteration: 134	 Weight: [23.65107428]	 Bias: [-58.08054305]	 Cost: 669.5051395620732
Iteration: 135	 Weight: [23.50381879]	 Bias: [-58.09244414]	 Cost: 648.053263122739
Iteration: 136	 Weight: [23.36157875]	 Bias: [-58.10356171]	 Cost: 628.0460471725489
Iteration: 137	 Weight: [23.22418135]	 Bias: [-58.11392282]	 Cost: 609.3856534020008
Iteration: 138	 Weight: [23.09145974]	 Bias: [-58.12355361]	 Cost: 591.980869709448
Iteration: 139	 Weight: [22.96325282]	 Bias: [-58.1324793]	 Cost: 575.7466614337657
Iteration: 140	 Weight: [22.83940504]	 Bias: [-58.14072427]	 Cost: 560.6037529802807
Iteration: 141	 Weight: [22.71976621]	 Bias: [-58.14831202]	 Cost: 546.4782377815607
Iteration: 142	 Weight: [22.60419131]	 Bias: [-58.15526529]	 Cost: 533.3012146740389
Iteration: 143	 Weight: [22.49254032]	 Bias: [-58.16160598]	 Cost: 521.0084489014432
Iteration: 144	 Weight: [22.38467805]	 Bias: [-58.16735529]	 Cost: 509.5400560771407
Iteration: 145	 Weight: [22.28047396]	 Bias: [-58.17253365]	 Cost: 498.8402075504947
Iteration: 146	 Weight: [22.17980202]	 Bias: [-58.1771608]	 Cost: 488.85685572760565
Iteration: 147	 Weight: [22.08254052]	 Bias: [-58.1812558]	 Cost: 479.54147799501953
Iteration: 148	 Weight: [21.98857197]	 Bias: [-58.18483706]	 Cost: 470.848837986484
Iteration: 149	 Weight: [21.8977829]	 Bias: [-58.18792234]	 Cost: 462.73676301818284
Iteration: 150	 Weight: [21.81006379]	 Bias: [-58.1905288]	 Cost: 455.1659365974189
Iteration: 151	 Weight: [21.72530886]	 Bias: [-58.192673]	 Cost: 448.0997049838749
Iteration: 152	 Weight: [21.64341597]	 Bias: [-58.19437095]	 Cost: 441.5038968517347
Iteration: 153	 Weight: [21.56428654]	 Bias: [-58.19563808]	 Cost: 435.34665516538297
Iteration: 154	 Weight: [21.48782534]	 Bias: [-58.1964893]	 Cost: 429.598280441515
Iteration: 155	 Weight: [21.41394045]	 Bias: [-58.19693901]	 Cost: 424.23108462649554
Iteration: 156	 Weight: [21.34254311]	 Bias: [-58.19700112]	 Cost: 419.2192548700338
Iteration: 157	 Weight: [21.27354762]	 Bias: [-58.19668905]	 Cost: 414.5387265249418
Iteration: 158	 Weight: [21.20687122]	 Bias: [-58.19601575]	 Cost: 410.16706474812463
Iteration: 159	 Weight: [21.14243402]	 Bias: [-58.19499374]	 Cost: 406.0833541202757
Iteration: 160	 Weight: [21.08015888]	 Bias: [-58.1936351]	 Cost: 402.26809574120296
Iteration: 161	 Weight: [21.0199713]	 Bias: [-58.1919515]	 Cost: 398.70311129448794
Iteration: 162	 Weight: [20.96179937]	 Bias: [-58.18995419]	 Cost: 395.37145360947295
Iteration: 163	 Weight: [20.90557365]	 Bias: [-58.18765407]	 Cost: 392.25732328053914
Iteration: 164	 Weight: [20.85122708]	 Bias: [-58.18506162]	 Cost: 389.3459909334355
Iteration: 165	 Weight: [20.79869493]	 Bias: [-58.18218699]	 Cost: 386.6237247562092
Iteration: 166	 Weight: [20.74791469]	 Bias: [-58.17903996]	 Cost: 384.0777229381827
Iteration: 167	 Weight: [20.698826]	 Bias: [-58.17563]	 Cost: 381.6960506845789
Iteration: 168	 Weight: [20.65137059]	 Bias: [-58.17196621]	 Cost: 379.46758149689987
Iteration: 169	 Weight: [20.60549219]	 Bias: [-58.16805743]	 Cost: 377.38194243015744
Iteration: 170	 Weight: [20.56113648]	 Bias: [-58.16391215]	 Cost: 375.4294630576206
Iteration: 171	 Weight: [20.51825099]	 Bias: [-58.1595386]	 Cost: 373.60112789198035
Iteration: 172	 Weight: [20.47678508]	 Bias: [-58.15494469]	 Cost: 371.88853202884593
Iteration: 173	 Weight: [20.43668985]	 Bias: [-58.1501381]	 Cost: 370.2838397943344
Iteration: 174	 Weight: [20.39791807]	 Bias: [-58.14512622]	 Cost: 368.7797461932977
Iteration: 175	 Weight: [20.36042416]	 Bias: [-58.13991618]	 Cost: 367.3694409685135
Iteration: 176	 Weight: [20.32416409]	 Bias: [-58.13451489]	 Cost: 366.04657509400516
Iteration: 177	 Weight: [20.28909537]	 Bias: [-58.12892899]	 Cost: 364.8052295376408
Iteration: 178	 Weight: [20.25517695]	 Bias: [-58.12316492]	 Cost: 363.63988613932037
Iteration: 179	 Weight: [20.2223692]	 Bias: [-58.11722889]	 Cost: 362.5454004614677
Iteration: 180	 Weight: [20.19063387]	 Bias: [-58.11112687]	 Cost: 361.51697647825705
Iteration: 181	 Weight: [20.15993401]	 Bias: [-58.10486467]	 Cost: 360.5501429790366
Iteration: 182	 Weight: [20.13023396]	 Bias: [-58.09844786]	 Cost: 359.64073156985893
Iteration: 183	 Weight: [20.10149928]	 Bias: [-58.09188183]	 Cost: 358.784856164881
Iteration: 184	 Weight: [20.07369671]	 Bias: [-58.08517181]	 Cost: 357.9788938667331
Iteration: 185	 Weight: [20.04679415]	 Bias: [-58.07832281]	 Cost: 357.2194671417841
Iteration: 186	 Weight: [20.02076059]	 Bias: [-58.07133969]	 Cost: 356.50342720261006
Iteration: 187	 Weight: [19.99556611]	 Bias: [-58.06422713]	 Cost: 355.8278385159011
Iteration: 188	 Weight: [19.9711818]	 Bias: [-58.05698967]	 Cost: 355.18996435958826
Iteration: 189	 Weight: [19.94757975]	 Bias: [-58.04963167]	 Cost: 354.5872533581303
Iteration: 190	 Weight: [19.92473303]	 Bias: [-58.04215736]	 Cost: 354.01732692971433
Iteration: 191	 Weight: [19.90261561]	 Bias: [-58.0345708]	 Cost: 353.4779675836061
Iteration: 192	 Weight: [19.88120239]	 Bias: [-58.02687594]	 Cost: 352.9671080100777
Iteration: 193	 Weight: [19.86046909]	 Bias: [-58.01907656]	 Cost: 352.48282090923186
Iteration: 194	 Weight: [19.84039231]	 Bias: [-58.01117635]	 Cost: 352.0233095086789
Iteration: 195	 Weight: [19.82094944]	 Bias: [-58.00317882]	 Cost: 351.586898723417
Iteration: 196	 Weight: [19.80211864]	 Bias: [-57.99508742]	 Cost: 351.1720269144161
Iteration: 197	 Weight: [19.78387884]	 Bias: [-57.98690544]	 Cost: 350.77723820536596
Iteration: 198	 Weight: [19.76620968]	 Bias: [-57.97863605]	 Cost: 350.4011753197756
Iteration: 199	 Weight: [19.7490915]	 Bias: [-57.97028236]	 Cost: 350.0425729031911
Weight: [19.7490915] Bias: [-57.97028236]
In [0]:

In [22]:
iterations=[1,2,3,4,5,6,7,8,9,10]
cost =[102.91008189424169,91.13317278598234,80.94548387910852,72.13255889976814,64.50888123548864,57.91396869153091,52.2089952377883,47.27386863183723,43.00470240023128,39.31162896162283]
plt.plot(regressor.it,regressor.cost_trend)
plt.xlabel('iterations')
plt.ylabel('cost')
plt.show()

In [0]:
