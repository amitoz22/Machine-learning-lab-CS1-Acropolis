The training process consists of the following steps:

Forward Propagation: Take the inputs, multiply by the weights (just use random numbers as weights) Let Y = Wi Ii = W1 I1+W2 I2+W3 I3 Pass the result through a sigmoid formula to calculate the neuronâ€™s output. The Sigmoid function is used to normalise the result between 0 and 1: 1/1 + e-y

Back Propagation

Calculate the error i.e the difference between the actual output and the expected output. Depending on the error, adjust the weights by multiplying the error with the input and again with the gradient of the Sigmoid curve: Weight += Error Input derivative of sigmoid ,here Output (1-Output) is derivative of sigmoid curve.

In [9]:
from numpy import *
  
class NeuralNet(object): 
    def __init__(self): 
        # Generate random numbers 
        #seed is used to diffrentiate b/w wt
        random.seed(1) 
  
        # Assign random weights to a 3 x 1 matrix, 
        self.synaptic_weights =  random.random((3,1)) 
  
    # The Sigmoid function 
    def __sigmoid(self, x): 
        return 1 / (1 + exp(-x)) 
  
    # The derivative of the Sigmoid function. 
    # This is the gradient of the Sigmoid curve. 
    def __sigmoid_derivative(self, x): 
        return x * (1 - x) 
  
    # Train the neural network and adjust the weights each time. 
    def train(self, inputs, outputs, training_iterations): 
        for iteration in range(training_iterations): 
  
            # Pass the training set through the network. 
            output = self.learn(inputs) 
  
            # Calculate the error 
            error = outputs - output 
  
            # Adjust the weights by a factor 
            factor = dot(inputs.T, error * self.__sigmoid_derivative(output)) 
            self.synaptic_weights += factor 
  
    # The neural network thinks. 
    def learn(self, inputs): 
        return self.__sigmoid(dot(inputs, self.synaptic_weights)) 
  
if __name__ == "__main__": 
  
    #Initialize 
    neural_network = NeuralNet() 
  
    # The training set. 
    inputs = array([[0, 1, 1], [1, 0, 0], [1, 0, 1]]) 
    outputs = array([[1, 0, 1]]).T 
  
    # Train the neural network 
    neural_network.train(inputs, outputs, 10) 
  
    # Test the neural network with a test example. 
    print (neural_network.learn(array([1, 0, 1])) )
[0.70679334]
In [10]:
# Class to create a neural  
# network with single neuron 
class NeuralNetwork(): 
      
    def __init__(self): 
          
        # Using seed to make sure it'll   
        # generate same weights in every run 
        random.seed(1) 
          
        # 3x1 Weight matrix 
        self.weight_matrix = 2 * random.random((3, 1)) - 1
  
    # tanh as activation fucntion 
    def tanh(self, x): 
        return tanh(x)
  
    # derivative of tanh function. 
    # Needed to calculate the gradients. 
    def tanh_derivative(self, x): 
        return 1-tanh(x)**2
  
    # forward propagation 
    def forward_propagation(self, inputs): 
        return self.tanh(dot(inputs, self.weight_matrix)) 
      
    # training the neural network. 
    def train(self, train_inputs, train_outputs, 
                            num_train_iterations): 
                                  
        # Number of iterations we want to 
        # perform for this set of input. 
        for iteration in range(num_train_iterations): 
            output = self.forward_propagation(train_inputs) 
  
            # Calculate the error in the output. 
            error = train_outputs - output 
  
            # multiply the error by input and then  
            # by gradient of tanh funtion to calculate 
            # the adjustment needs to be made in weights 
            adjustment = dot(train_inputs.T, error *
                             self.tanh_derivative(output)) 
                               
            # Adjust the weight matrix 
            self.weight_matrix += adjustment 
  
# Driver Code 
if __name__ == "__main__": 
      
    neural_network = NeuralNetwork() 
      
    print ('Random weights at the start of training') 
    print (neural_network.weight_matrix) 
  
    train_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]]) 
    train_outputs = array([[0, 1, 1, 0]]).T 
  
    neural_network.train(train_inputs, train_outputs, 10) 
  
    print ('New weights after training') 
    print (neural_network.weight_matrix) 
  
    # Test the neural network with a new situation. 
    print ("Testing network on new examples ->")
    print (neural_network.forward_propagation(array([1, 0, 0])))
Random weights at the start of training
[[-0.16595599]
 [ 0.44064899]
 [-0.99977125]]
New weights after training
[[2.1623996 ]
 [0.07459826]
 [0.04407669]]
Testing network on new examples ->
[0.9738734]
In [12]:
# Class to create a neural  
# network with single neuron
import numpy as np
class NeuralNetwork(): 
      
    def __init__(self): 
          
        # Using seed to make sure it'll   
        # generate same weights in every run 
        random.seed(1) 
          
        # 3x1 Weight matrix 
        self.weight_matrix = 2 * random.random((3, 1)) - 1
  
    # tanh as activation fucntion 
    def relu(self, x): 
        return np.maximum(0,x)
  
    # derivative of tanh function. 
    # Needed to calculate the gradients. 
    def relu_derivative(self, x): 
        return np.where(x <= 0, 0, 1)
  
    # forward propagation 
    def forward_propagation(self, inputs): 
        return self.relu(dot(inputs, self.weight_matrix)) 
      
    # training the neural network. 
    def train(self, train_inputs, train_outputs, 
                            num_train_iterations): 
                                  
        # Number of iterations we want to 
        # perform for this set of input. 
        for iteration in range(num_train_iterations): 
            output = self.forward_propagation(train_inputs) 
  
            # Calculate the error in the output. 
            error = train_outputs - output 
  
            # multiply the error by input and then  
            # by gradient of tanh funtion to calculate 
            # the adjustment needs to be made in weights 
            adjustment = dot(train_inputs.T, error *
                             self.relu_derivative(output)) 
                               
            # Adjust the weight matrix 
            self.weight_matrix += adjustment 
  
# Driver Code 
if __name__ == "__main__": 
      
    neural_network = NeuralNetwork() 
      
    print ('Random weights at the start of training') 
    print (neural_network.weight_matrix) 
  
    train_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]]) 
    train_outputs = array([[0, 1, 1, 0]]).T 
  
    neural_network.train(train_inputs, train_outputs, 10) 
  
    print ('New weights after training') 
    print (neural_network.weight_matrix) 
  
    # Test the neural network with a new situation. 
    print ("Testing network on new examples ->")
    print (neural_network.forward_propagation(array([1,0,1])))
Random weights at the start of training
[[-0.16595599]
 [ 0.44064899]
 [-0.99977125]]
New weights after training
[[-0.16595599]
 [ 0.44064899]
 [-0.99977125]]
Testing network on new examples ->
[0.]
